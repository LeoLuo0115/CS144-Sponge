{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoLuo0115/CS144-Sponge/blob/master/%E2%80%9CNLP_Project1_ipynb%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGQk7O27KkGX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Unigram:\n",
        "  def __init__(self):\n",
        "    self.unigramCounts = {\"<unk>\":0} #stores unigram[token] counts make sure UNK already initialized to 0\n",
        "    self.vocabSet = set() #stores total vocab size\n",
        "    self.totalCount = 0\n",
        "\n",
        "#Increase count of a token or adds it to dictionary\n",
        "  def addToken(self, token):\n",
        "    if token not in self.unigramCounts: #If not already in the dictionary add it in with a count of 1 and increase the vocab size\n",
        "      self.unigramCounts[token] = 0\n",
        "      self.vocabSet.add(token)\n",
        "    self.unigramCounts[token] += 1 #If already in dictionary increment its count by 1\n",
        "    self.totalCount +=1\n",
        "\n",
        "#Gets token count if it can't find it return the count of UNK unkown\n",
        "  def getTokenCount(self, token):\n",
        "    token = token.lower()\n",
        "    if token not in self.unigramCounts:\n",
        "      return self.unigramCounts[\"<unk>\"]\n",
        "    else:\n",
        "      return self.unigramCounts[token]\n",
        "\n",
        "#Converts an input line of space delineated text into a readable token array\n",
        "  def convertLineToTokenArray(self, line):\n",
        "    tokens = line.lower().split()\n",
        "    tokens = [\"<s>\"] + tokens + [\"</s>\"] #add start and stop tokens to beginning and end of line\n",
        "    return tokens\n",
        "\n",
        "#Any token counts under the threshold are removed from the dictionary and their counts added to the UNK count\n",
        "  def handleUnknownTokens(self, threshold):\n",
        "    listOfTokens = list(self.unigramCounts.keys())\n",
        "    listOfTokens.remove(\"<unk>\")\n",
        "    for token in listOfTokens:\n",
        "      if self.getTokenCount(token) <= threshold:\n",
        "        self.unigramCounts[\"<unk>\"] += self.getTokenCount(token)\n",
        "        del self.unigramCounts[token]\n",
        "\n",
        "  def addArrayToUnigram(self, tokens):\n",
        "    for token in tokens:\n",
        "      self.addToken(token)\n",
        "\n",
        "  def getVocabSize(self):\n",
        "    return len(self.vocabSet)\n",
        "\n",
        "  def trainFileGetCounts(self, filename):\n",
        "    with open(filename, \"r\") as file:\n",
        "      for line in file:\n",
        "        tokenArray = self.convertLineToTokenArray(line)\n",
        "        self.addArrayToUnigram(tokenArray)\n",
        "\n",
        "  def getLaplaceSmoothedTokenProb(self, token, smoothAmount, vocabSize):\n",
        "    return ((self.getTokenCount(token) + smoothAmount) / ((self.totalCount) + (smoothAmount*vocabSize)))\n",
        "\n",
        "  def getPerplexityOnFile(self, filePath, smoothAmount):\n",
        "    with open(filePath, \"r\") as file:\n",
        "      totalLogProb = 0\n",
        "      totalTokens = 0\n",
        "      for line in file:\n",
        "        tokenArray = self.convertLineToTokenArray(line)\n",
        "        for token in tokenArray:\n",
        "          prob = self.getLaplaceSmoothedTokenProb(token, smoothAmount, self.getVocabSize())\n",
        "          totalLogProb += math.log(prob)\n",
        "          totalTokens += 1\n",
        "      avgLogProb = totalLogProb / totalTokens\n",
        "      perplexity = math.exp(-avgLogProb)\n",
        "\n",
        "      return perplexity\n",
        "\n",
        "  def addCount(self, token, count):\n",
        "    if token not in self.unigramCounts:\n",
        "      self.unigramCounts[token] = 0\n",
        "      self.vocabSet.add(token)\n",
        "    self.totalCount += count\n",
        "    self.unigramCounts[token] += count\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bigram:\n",
        "  def __init__(self):\n",
        "    self.bigramCounts = {\"<unk>\":Unigram()}\n",
        "    self.vocabSet = set()\n",
        "\n",
        "  def convertLineToTokenArray(self, line):\n",
        "    tokens = line.lower().split()\n",
        "    tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
        "    return tokens\n",
        "\n",
        "  def addBigramTokens(self, firstToken, secondToken):\n",
        "    if firstToken not in self.bigramCounts:\n",
        "      self.bigramCounts[firstToken] = Unigram()\n",
        "      self.vocabSet.add(firstToken)\n",
        "    self.bigramCounts[firstToken].addToken(secondToken)\n",
        "\n",
        "  def addArrayToBigram(self, tokens):\n",
        "    for i in range(len(tokens)-1):\n",
        "      self.addBigramTokens(tokens[i], tokens[i+1])\n",
        "\n",
        "  def trainFileGetCounts(self, filePath):\n",
        "    with open(filePath, \"r\") as file:\n",
        "      for line in file:\n",
        "        tokenArray = self.convertLineToTokenArray(line)\n",
        "        self.addArrayToBigram(tokenArray)\n",
        "\n",
        "  def getUnigram(self, firstToken):\n",
        "    firstToken = firstToken.lower()\n",
        "    if firstToken not in self.bigramCounts:\n",
        "      return self.bigramCounts[\"<unk>\"]\n",
        "    else:\n",
        "      return self.bigramCounts[firstToken]\n",
        "\n",
        "  def getLSmoothedTokenProb(self, firstToken, secondToken, smoothAmount):\n",
        "    vocabSize = len(self.vocabSet)\n",
        "    unigram = self.getUnigram(firstToken)\n",
        "    return unigram.getLaplaceSmoothedTokenProb(secondToken, smoothAmount, vocabSize)\n",
        "\n",
        "  def getPerplexityOnFile(self, filePath, smoothing):\n",
        "    with open(filePath, \"r\") as file:\n",
        "      totalLogProb = 0\n",
        "      totalTokens = 0\n",
        "      for line in file:\n",
        "        tokenArray = self.convertLineToTokenArray(line)\n",
        "        for i in range(len(tokenArray)-1):\n",
        "          prob = self.getLSmoothedTokenProb(tokenArray[i], tokenArray[i+1], smoothing)\n",
        "          totalLogProb += math.log(prob)\n",
        "          totalTokens += 1\n",
        "      avgLogProb = totalLogProb / totalTokens\n",
        "      perplexity = math.exp(-avgLogProb)\n",
        "\n",
        "      return perplexity\n",
        "\n",
        "  def handleUnknownTokens(self, threshold1, threshold2):\n",
        "    listOfFirstTokens = list(self.bigramCounts.keys())\n",
        "    listOfFirstTokens.remove(\"<unk>\")\n",
        "    for firstToken in listOfFirstTokens:\n",
        "      unigram = self.getUnigram(firstToken)\n",
        "      if unigram.totalCount <= threshold1:\n",
        "        unigramTokens = list(unigram.unigramCounts.keys())\n",
        "        for secondToken in unigramTokens:\n",
        "          count=unigram.getTokenCount(secondToken)\n",
        "          self.bigramCounts[\"<unk>\"].addCount(secondToken, count)\n",
        "\n",
        "        del self.bigramCounts[firstToken]\n",
        "    for firstToken in self.bigramCounts:\n",
        "      self.bigramCounts[firstToken].handleUnknownTokens(threshold2)\n",
        "\n"
      ],
      "metadata": {
        "id": "KdTOmQvONexn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = Unigram()\n",
        "unigram.trainFileGetCounts(\"/content/drive/MyDrive/NLP_HW/train.txt\")\n",
        "unigram.handleUnknownTokens(1)\n",
        "print(unigram.getLaplaceSmoothedTokenProb('The',5, unigram.getVocabSize()))\n",
        "print(unigram.getPerplexityOnFile(\"/content/drive/MyDrive/NLP_HW/train.txt\", 1))\n",
        "print(unigram.getPerplexityOnFile(\"/content/drive/MyDrive/NLP_HW/val.txt\", 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0O61Mbvi6Ip",
        "outputId": "3a5eae23-c2e9-495a-9fd9-6704e1a939e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04319920403203445\n",
            "347.1096958708179\n",
            "304.7265083166604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Bigram()\n",
        "bigram.trainFileGetCounts(\"/content/drive/MyDrive/NLP_HW/train.txt\")\n",
        "bigram.handleUnknownTokens(20, 5)\n",
        "print(bigram.getPerplexityOnFile(\"/content/drive/MyDrive/NLP_HW/train.txt\", 2))\n"
      ],
      "metadata": {
        "id": "3NZhhTmSV5HD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}